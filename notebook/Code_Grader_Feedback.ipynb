{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-18T09:59:48.874656Z","iopub.status.busy":"2024-09-18T09:59:48.874007Z","iopub.status.idle":"2024-09-18T09:59:49.244006Z","shell.execute_reply":"2024-09-18T09:59:49.243116Z","shell.execute_reply.started":"2024-09-18T09:59:48.874616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/llama-2/transformers/default/3/adapter_model.safetensors\n","/kaggle/input/llama-2/transformers/default/3/adapter_config.json\n","/kaggle/input/llama-2/transformers/default/3/tokenizer.model\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T09:59:52.800000Z","iopub.status.busy":"2024-09-18T09:59:52.798913Z","iopub.status.idle":"2024-09-18T10:00:13.168171Z","shell.execute_reply":"2024-09-18T10:00:13.167002Z","shell.execute_reply.started":"2024-09-18T09:59:52.799942Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting trl\n","  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n","Collecting peft\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\n","Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.44.0)\n","Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\n","Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Downloading trl-0.10.1-py3-none-any.whl (280 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, tyro, bitsandbytes, trl, peft\n","Successfully installed bitsandbytes-0.43.3 peft-0.12.0 shtab-1.7.1 trl-0.10.1 tyro-0.8.10\n"]}],"source":["!pip install trl peft datasets accelerate bitsandbytes"]},{"cell_type":"markdown","metadata":{},"source":["### Apply huggingface login for Dataset download"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:00:13.171346Z","iopub.status.busy":"2024-09-18T10:00:13.170522Z","iopub.status.idle":"2024-09-18T10:00:14.853510Z","shell.execute_reply":"2024-09-18T10:00:14.852358Z","shell.execute_reply.started":"2024-09-18T10:00:13.171290Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token <your-hugging-face-token>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:00:16.543917Z","iopub.status.busy":"2024-09-18T10:00:16.542921Z","iopub.status.idle":"2024-09-18T10:00:38.495853Z","shell.execute_reply":"2024-09-18T10:00:38.494638Z","shell.execute_reply.started":"2024-09-18T10:00:16.543865Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:01:54.661136Z","iopub.status.busy":"2024-09-18T10:01:54.660387Z","iopub.status.idle":"2024-09-18T10:01:54.665811Z","shell.execute_reply":"2024-09-18T10:01:54.664720Z","shell.execute_reply.started":"2024-09-18T10:01:54.661092Z"},"trusted":true},"outputs":[],"source":["model_name = \"meta-llama/CodeLlama-7b-Instruct-hf\""]},{"cell_type":"markdown","metadata":{},"source":["#### Loading and Streaming a Dataset\n","\n","In this code cell, we use the `datasets` library to load a large dataset from the \"bigcode/the-stack\" collection. Specifically, the dataset is filtered to include only Python files located in the `\"data/python\"` directory, and we load the `train` split. The `streaming=True` option allows us to efficiently handle this large dataset without needing to load everything into memory all at once.\n","\n","To keep track of the number of files processed, we initialize a `file_count` variable and limit the process to a maximum of 80,000 files (`max_files`). The list `downloaded_samples` is created to store the files we retrieve.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:02:06.943687Z","iopub.status.busy":"2024-09-18T10:02:06.943008Z","iopub.status.idle":"2024-09-18T10:02:10.809344Z","shell.execute_reply":"2024-09-18T10:02:10.808354Z","shell.execute_reply.started":"2024-09-18T10:02:06.943650Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"083c772443bc4b14870373ecbf9b6075","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/19.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46624e2519df462cb10c0309542c7231","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","# Import the load_dataset function from the datasets library to load a specific dataset.\n","# Stream the dataset \"bigcode/the-stack\" filtered for the \"python\" directory, and load only the training split.\n","# Enable streaming to handle large datasets efficiently without loading the entire dataset into memory at once.\n","ds = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", streaming=True, split=\"train\")\n","\n","# Initialize a counter for the number of files processed.\n","file_count = 0\n","\n","# Set the maximum number of files to process (80,000 in this case).\n","max_files = 80000\n","\n","# Create an empty list to store the downloaded samples.\n","downloaded_samples = []"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:02:14.243840Z","iopub.status.busy":"2024-09-18T10:02:14.242948Z","iopub.status.idle":"2024-09-18T10:04:25.125534Z","shell.execute_reply":"2024-09-18T10:04:25.124560Z","shell.execute_reply.started":"2024-09-18T10:02:14.243800Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished downloading 80000 files.\n"]}],"source":["# Iterate over the dataset and stop after downloading the content\n","for sample in iter(ds):\n","    content = sample[\"content\"]  # Accessing the file content\n","    downloaded_samples.append(content)\n","    file_count += 1\n","    if file_count >= max_files:\n","        break\n","\n","print(f\"Finished downloading {file_count} files.\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating a Dataset from Downloaded Samples\n","\n","In this code, we first import the `Dataset` and `DatasetDict` classes from the `datasets` library. These classes allow us to manipulate and create datasets in a structured manner.\n","\n","We then create a dictionary `data_dict`, where the key is `\"content\"` and the value is the list of `downloaded_samples`. This list contains the samples we've processed from the original dataset. \n","\n","Using `Dataset.from_dict()`, we convert the dictionary into a `Dataset` object, which is a format used by Hugging Face's library to handle datasets efficiently. This new dataset can be further processed, explored, or saved.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:05:55.160810Z","iopub.status.busy":"2024-09-18T10:05:55.160408Z","iopub.status.idle":"2024-09-18T10:06:01.135106Z","shell.execute_reply":"2024-09-18T10:06:01.133980Z","shell.execute_reply.started":"2024-09-18T10:05:55.160772Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","\n","# Import Dataset and DatasetDict classes from the datasets library to create and manage datasets.\n","\n","# Create a dictionary with the key \"content\" and assign the list of downloaded samples to it.\n","data_dict = {\"content\": downloaded_samples}\n","\n","# Convert the dictionary into a Hugging Face Dataset object using Dataset.from_dict.\n","new_dataset = Dataset.from_dict(data_dict)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:06:01.137597Z","iopub.status.busy":"2024-09-18T10:06:01.137192Z","iopub.status.idle":"2024-09-18T10:06:01.142508Z","shell.execute_reply":"2024-09-18T10:06:01.141513Z","shell.execute_reply.started":"2024-09-18T10:06:01.137552Z"},"trusted":true},"outputs":[],"source":["# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = True\n","bf16 = False\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Load the entire model on the GPU 0\n","device_map = \"auto\""]},{"cell_type":"markdown","metadata":{},"source":["#### Loading a Tokenizer and Model with QLoRA Configuration\n","\n","This code demonstrates how to load a model using QLoRA (Quantized Low-Rank Adaptation) with a custom configuration for efficient memory usage and computation.\n","\n","1. **Compute Type Setup**: The compute precision is set to `float16`, which allows for reduced memory usage and faster computations, especially on GPUs.\n","\n","2. **BitsAndBytes Configuration**: The `BitsAndBytesConfig` object is created to load the model in 4-bit precision. This quantization reduces the size of the model while maintaining good performance. The `nf4` quantization type is used, and double quantization is disabled to avoid potential overhead.\n","\n","3. **Loading the Model**: \n","   - The base model is loaded using `AutoModelForCausalLM.from_pretrained()` with options for low CPU memory usage and efficient model loading.\n","   - The model is loaded in `float16` precision and mapped to the appropriate device (`device_map`) for inference (usually a GPU if available).\n","\n","4. **Config Settings**:\n","   - Caching of model outputs is disabled (`use_cache = False`) to optimize fine-tuning.\n","   - Pretraining tensor parallelism (`pretraining_tp = 1`) is set to manage multi-GPU environments during model training or fine-tuning."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:06:08.481715Z","iopub.status.busy":"2024-09-18T10:06:08.481359Z","iopub.status.idle":"2024-09-18T10:10:26.859239Z","shell.execute_reply":"2024-09-18T10:10:26.858358Z","shell.execute_reply.started":"2024-09-18T10:06:08.481682Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bd7019cc79043eb8a6d326feb24e13a","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fd2c83e56a94ef98b4b53f48dcb8ac5","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:  50%|####9     | 4.94G/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccd8746ff1834137b1a8cccfb8c628ea","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"132cc9cd91754fb1bdb143650f48aeb7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9553cdfdb060421481798f5ceff9da6b","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Set the compute precision to \"float16\" using PyTorch's built-in types.\n","compute_dtype = getattr(torch, \"float16\")\n","\n","# Define the configuration for the BitsAndBytes quantization.\n","# Load the model in 4-bit precision with 'nf4' quantization type for efficient model size reduction.\n","# Set the computation precision to \"float16\" and disable double quantization for performance balance.\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=\"float16\",\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","# Load the base model for causal language modeling from Hugging Face's library.\n","# Use the low CPU memory usage mode to reduce memory load during the loading process.\n","# Specify the compute precision (float16) and map the model to the appropriate device (CPU or GPU) for inference.\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n","    quantization_config=bnb_config  # Pass the BitsAndBytes configuration here\n",")\n","\n","# Disable caching for model outputs, and set pretraining tensor parallelism to 1 for more efficient fine-tuning.\n","base_model.config.use_cache = False\n","base_model.config.pretraining_tp = 1"]},{"cell_type":"markdown","metadata":{},"source":["#### Loading the LLaMA Tokenizer and Tokenizing the Dataset\n","\n","1. **Loading the Tokenizer**:\n","   - The `AutoTokenizer.from_pretrained()` function is used to load the tokenizer for the model (in this case, a LLaMA tokenizer).\n","   - The option `trust_remote_code=True` allows the loading of any custom tokenizer logic from the model's repository.\n","\n","2. **Setting Padding Token**:\n","   - Since some models don't have an explicit padding token, we set the `pad_token` to the end-of-sequence token (`eos_token`) to manage padding during tokenization.\n","   - The `padding_side` is set to `\"right\"` to ensure the model doesn't encounter overflow issues during training with fp16 precision, which may occur if padding is on the left.\n","\n","3. **Tokenization Function**:\n","   - The `tokenize_function` is defined to tokenize each example in the dataset. We use truncation to ensure that each tokenized sequence has a maximum length of 512 tokens.\n","   \n","4. **Applying Tokenization**:\n","   - We use the `.map()` method to apply the tokenization function to the entire dataset. Setting `batched=True` processes multiple examples at once, which speeds up the tokenization process."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:10:26.861702Z","iopub.status.busy":"2024-09-18T10:10:26.861109Z","iopub.status.idle":"2024-09-18T10:14:01.251372Z","shell.execute_reply":"2024-09-18T10:14:01.250527Z","shell.execute_reply.started":"2024-09-18T10:10:26.861655Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c79e2d48b27543bba8cabbff293d85fa","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0757398488b40549c99a2af7039ae2d","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c51d04cd5272425cabaac4fbf24f8fc5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e75f608367eb418d9569386f342a8a63","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f33e07309cc545e7887c1895404f6d5a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load LLaMA tokenizer from the specified model name\n","# 'trust_remote_code=True' ensures that custom tokenization logic from the model repository is trusted.\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","\n","# Set the pad token to be the same as the end-of-sequence (EOS) token to handle padding properly.\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Define the padding side as \"right\" to avoid issues during fp16 training where overflow might occur.\n","tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n","\n","# Define a function that tokenizes each example in the dataset, truncating them to a maximum length of 512 tokens.\n","def tokenize_function(examples):\n","    return tokenizer(examples['content'], truncation=True, max_length=512)\n","\n","# Apply the tokenization function to the entire dataset using map for batched processing to improve speed.\n","tokenized_dataset = new_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### Loading LoRA Configuration and Fine-Tuning the Model\n","\n","1. **LoRA Configuration**:\n","   - LoRA (Low-Rank Adaptation) is a technique that reduces the number of trainable parameters by introducing low-rank matrices into certain layers of the model. \n","   - We define `LoraConfig` with key parameters like:\n","     - `lora_alpha=16`: A scaling factor to adjust the importance of LoRA layers.\n","     - `lora_dropout=0.1`: Dropout applied to LoRA layers during training to prevent overfitting.\n","     - `r=64`: The rank for the low-rank matrices.\n","     - `task_type=\"CAUSAL_LM\"`: Specifies the task as causal language modeling.\n","\n","2. **Training Arguments**:\n","   - The `TrainingArguments` class defines the parameters for the training process:\n","     - **Batch size**: Set to 4 samples per device.\n","     - **Optimizer**: `paged_adamw_32bit`, a memory-efficient variant of AdamW.\n","     - **Learning rate**: Set to `2e-4`.\n","     - **Mixed precision**: Use either `fp16` or `bf16` for faster and more efficient training on supported hardware.\n","     - **Gradient clipping**: Ensures stable gradients with `max_grad_norm=0.3`.\n","     - **Logging and saving**: Logs training progress every 25 steps but does not save intermediate checkpoints.\n","     - **Cosine learning rate scheduler**: Provides smooth learning rate decay over time.\n","\n","3. **Supervised Fine-Tuning (SFT) Setup**:\n","   - `SFTTrainer` is initialized with the pre-trained model, tokenized dataset, and LoRA configuration for parameter-efficient fine-tuning.\n","   - `dataset_text_field=\"content\"` specifies the text field in the dataset, while `packing=False` disables sequence packing.\n","\n","4. **Training**:\n","   - The `trainer.train()` call starts the training process with the defined configuration and parameters"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T10:14:31.163997Z","iopub.status.busy":"2024-09-18T10:14:31.163151Z","iopub.status.idle":"2024-09-18T11:45:02.895502Z","shell.execute_reply":"2024-09-18T11:45:02.894559Z","shell.execute_reply.started":"2024-09-18T10:14:31.163956Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 1:29:40, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>0.787000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.825400</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.746900</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.898700</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.806900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.775400</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.726300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.860900</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.774100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.781900</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.731300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.867900</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>0.722200</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.963400</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.749700</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.878200</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>0.735600</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.798000</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>0.786200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.860500</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>0.797200</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.857400</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>0.772200</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.863000</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.755000</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.931500</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>0.817900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.832300</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.797900</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>0.779700</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.829200</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>0.705700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.705200</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>0.743900</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.786800</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>0.732700</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.827500</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>0.687600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.787300</td>\n","    </tr>\n","    <tr>\n","      <td>1025</td>\n","      <td>0.754800</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.860200</td>\n","    </tr>\n","    <tr>\n","      <td>1075</td>\n","      <td>0.721900</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.775100</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>0.746200</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.823000</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>0.752800</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.917800</td>\n","    </tr>\n","    <tr>\n","      <td>1225</td>\n","      <td>0.668700</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.839300</td>\n","    </tr>\n","    <tr>\n","      <td>1275</td>\n","      <td>0.770700</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.904700</td>\n","    </tr>\n","    <tr>\n","      <td>1325</td>\n","      <td>0.774000</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.759100</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>0.751500</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.801000</td>\n","    </tr>\n","    <tr>\n","      <td>1425</td>\n","      <td>0.767300</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.821000</td>\n","    </tr>\n","    <tr>\n","      <td>1475</td>\n","      <td>0.753300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.875000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1500, training_loss=0.7948865458170573, metrics={'train_runtime': 5385.8626, 'train_samples_per_second': 1.114, 'train_steps_per_second': 0.279, 'total_flos': 9.981113253750374e+16, 'train_loss': 0.7948865458170573, 'epoch': 0.075})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Load LoRA (Low-Rank Adaptation) configuration for fine-tuning the model\n","# LoRA parameters include alpha scaling, dropout rate, rank (r), and bias handling.\n","peft_config = LoraConfig(\n","    lora_alpha=16,             # Scaling factor for LoRA layers\n","    lora_dropout=0.1,          # Dropout rate applied to LoRA layers during training\n","    r=64,                      # Rank of the low-rank adaptation matrices\n","    bias=\"none\",               # No additional bias parameters\n","    task_type=\"CAUSAL_LM\",      # The task type for causal language modeling\n",")\n","\n","# Set the training arguments for fine-tuning\n","training_arguments = TrainingArguments(\n","    output_dir=\"./results\",             # Directory to save the model and other results\n","    num_train_epochs=1,                 # Number of training epochs\n","    per_device_train_batch_size=4,      # Batch size per device (GPU/CPU)\n","    gradient_accumulation_steps=1,      # Number of steps to accumulate gradients before updating weights\n","    optim=\"paged_adamw_32bit\",          # Optimizer: paged AdamW, a memory-efficient variant\n","    save_steps=0,                       # Do not save checkpoints during training\n","    logging_steps=25,                   # Log training metrics every 25 steps\n","    learning_rate=2e-4,                 # Learning rate for the optimizer\n","    weight_decay=0.001,                 # Weight decay to regularize the model\n","    fp16=fp16,                          # Enable mixed precision training with fp16 (if available)\n","    bf16=bf16,                          # Enable bf16 precision training (if supported by hardware)\n","    max_grad_norm=0.3,                  # Maximum gradient norm for gradient clipping\n","    max_steps=1500,                     # Maximum number of training steps\n","    warmup_ratio=0.03,                  # Warmup ratio for learning rate schedule\n","    group_by_length=True,               # Group samples by length for more efficient training\n","    lr_scheduler_type=\"cosine\",         # Use cosine learning rate scheduler\n","    report_to=\"tensorboard\",            # Report training progress to TensorBoard\n",")\n","\n","# Initialize the supervised fine-tuning trainer (SFTTrainer) with the model, dataset, and LoRA configuration\n","trainer = SFTTrainer(\n","    model=base_model,                   # Pre-trained model to fine-tune\n","    train_dataset=tokenized_dataset,    # Tokenized training dataset\n","    peft_config=peft_config,            # LoRA configuration for parameter-efficient fine-tuning\n","    dataset_text_field=\"content\",       # Field in the dataset containing the text content\n","    max_seq_length=None,                # No fixed sequence length for input data\n","    tokenizer=tokenizer,                # Tokenizer used for processing the data\n","    args=training_arguments,            # Training arguments specified earlier\n","    packing=False,                      # Disable sequence packing (padding)\n",")\n","\n","# Start training the model\n","trainer.train()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:45:20.787506Z","iopub.status.busy":"2024-09-18T11:45:20.786819Z","iopub.status.idle":"2024-09-18T11:45:21.328246Z","shell.execute_reply":"2024-09-18T11:45:21.327432Z","shell.execute_reply.started":"2024-09-18T11:45:20.787464Z"},"trusted":true},"outputs":[],"source":["# Save trained model\n","trainer.model.save_pretrained(\"Lora-Llama-2-matrices-before-code-grader\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:52:55.892436Z","iopub.status.busy":"2024-09-18T11:52:55.891713Z","iopub.status.idle":"2024-09-18T11:52:55.896786Z","shell.execute_reply":"2024-09-18T11:52:55.895765Z","shell.execute_reply.started":"2024-09-18T11:52:55.892395Z"},"trusted":true},"outputs":[],"source":["new_model=\"/kaggle/input/lora-llama-2-matrices-before-code-grader/pytorch/default/1\""]},{"cell_type":"markdown","metadata":{},"source":["#### Merging LoRA Weights\n","   - We use `PeftModel.from_pretrained()` to load the LoRA-augmented model, combining the base model with the fine-tuned LoRA weights.\n","   - The method `merge_and_unload()` merges the LoRA layers back into the base model, removing the need to maintain separate LoRA layers, which simplifies inference and reduces memory consumption.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:53:01.328948Z","iopub.status.busy":"2024-09-18T11:53:01.328561Z","iopub.status.idle":"2024-09-18T11:53:08.977095Z","shell.execute_reply":"2024-09-18T11:53:08.976272Z","shell.execute_reply.started":"2024-09-18T11:53:01.328907Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n","  warnings.warn(\n"]}],"source":["# Load the LoRA-augmented model by combining the base model with the fine-tuned LoRA weights.\n","model = PeftModel.from_pretrained(base_model, new_model)\n","\n","# Merge the LoRA weights into the base model and unload them to free up memory.\n","model = model.merge_and_unload()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:54:51.728899Z","iopub.status.busy":"2024-09-18T11:54:51.728050Z","iopub.status.idle":"2024-09-18T11:55:05.156021Z","shell.execute_reply":"2024-09-18T11:55:05.155042Z","shell.execute_reply.started":"2024-09-18T11:54:51.728860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INST] Write the feedback of this model print(\"hello world\") [/INST]  The feedback for the model print(\"hello world\") is:\n","\n","* The model is a simple print statement that prints the string \"hello world\" to the console.\n","* The model is a good example of a basic Python program that can be used to print a string to the console.\n","* The model is well-structured and easy to understand, with a clear and concise syntax.\n","* The model is a good starting point for learning Python programming, as it covers the basic syntax and functionality of the language.\n","* The model is a good example of how to use the `print()` function in Python to print a string to the console.\n","* The model is a good example of how to use the `\"` character to enclose a string in Python.\n","\n","Overall, the model is a good example of a basic Python program that can be used to print\n"]}],"source":["logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"\"\"Write the feedback of this model print(\"hello world\")\"\"\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T12:15:11.960900Z","iopub.status.busy":"2024-09-18T12:15:11.960506Z","iopub.status.idle":"2024-09-18T12:15:11.965309Z","shell.execute_reply":"2024-09-18T12:15:11.964334Z","shell.execute_reply.started":"2024-09-18T12:15:11.960864Z"},"trusted":true},"outputs":[],"source":["new_merged_model = \"CodeLlama-2-fine-tuned-code-grader\""]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T11:56:47.172090Z","iopub.status.busy":"2024-09-18T11:56:47.171716Z","iopub.status.idle":"2024-09-18T11:57:02.929611Z","shell.execute_reply":"2024-09-18T11:57:02.928596Z","shell.execute_reply.started":"2024-09-18T11:56:47.172057Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('CodeLlama-2-fine-tuned-code-grader/tokenizer_config.json',\n"," 'CodeLlama-2-fine-tuned-code-grader/special_tokens_map.json',\n"," 'CodeLlama-2-fine-tuned-code-grader/tokenizer.model',\n"," 'CodeLlama-2-fine-tuned-code-grader/added_tokens.json',\n"," 'CodeLlama-2-fine-tuned-code-grader/tokenizer.json')"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Save the merged model\n","model.save_pretrained(new_merged_model)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(new_merged_model)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"isSourceIdPinned":true,"modelId":121761,"modelInstanceId":97574,"sourceId":116134,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
